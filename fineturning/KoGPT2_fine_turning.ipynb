{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KoGPT2_학습_소설112312312323.ipynb의 사본의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8_qH26dnbc_",
        "colab_type": "code",
        "outputId": "4f55e743-c1dc-4ec0-9aa2-3fa6a77bea85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "# GPU로 바꾼후 GPU연동 확인\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jun 14 10:50:33 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cebOpJ6ZRINJ",
        "colab_type": "text"
      },
      "source": [
        "########학습부분#############\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp-qHJNenjWp",
        "colab_type": "code",
        "outputId": "ae8a367e-7cfc-4bfe-ee29-2246fc4577de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# 구글 드라이브 마운트\n",
        "# 구글드라이브에 있는 데이터 파일과 kogpt2(구버전)사용\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do03LsGZnwtt",
        "colab_type": "code",
        "outputId": "eefea6a5-9d5e-4b30-9175-c57898cf57fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        }
      },
      "source": [
        "# 필수 패키지 설치\n",
        "!pip install -r /content/drive/'My Drive'/NarrativeKoGPT2/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gluonnlp==0.9.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 1)) (0.9.1)\n",
            "Requirement already satisfied: mxnet==1.6.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.85 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 3)) (0.1.91)\n",
            "Requirement already satisfied: torch==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 4)) (1.5.0+cu101)\n",
            "Collecting transformers==2.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from gluonnlp==0.9.1->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from gluonnlp==0.9.1->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 1)) (20.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from gluonnlp==0.9.1->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 1)) (0.29.19)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet==1.6.0->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 2)) (0.8.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet==1.6.0->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 4)) (0.16.0)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 5)) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 5)) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 5)) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 5)) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 5)) (0.0.43)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp==0.9.1->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->gluonnlp==0.9.1->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 2)) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 2)) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 5)) (0.15.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0->-r /content/drive/My Drive/NarrativeKoGPT2/requirements.txt (line 5)) (7.1.2)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Found existing installation: tokenizers 0.0.11\n",
            "    Uninstalling tokenizers-0.0.11:\n",
            "      Successfully uninstalled tokenizers-0.0.11\n",
            "  Found existing installation: transformers 2.4.0\n",
            "    Uninstalling transformers-2.4.0:\n",
            "      Successfully uninstalled transformers-2.4.0\n",
            "Successfully installed tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6ZGcUwuQCfI",
        "colab_type": "text"
      },
      "source": [
        "1. 시스템경로 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUbzAUUEd_92",
        "colab_type": "code",
        "outputId": "bf491d50-6dfb-432f-89c1-10a9a9768e5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install minegpt2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting minegpt2\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/1b/c1a420b0916953ef190df9e427963ac690decf7dfa18c8b99e885f82b08f/minegpt2-0.0.2-py3-none-any.whl\n",
            "Installing collected packages: minegpt2\n",
            "Successfully installed minegpt2-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrgHxperoEoP",
        "colab_type": "code",
        "outputId": "696da6b5-94b4-4fe3-dd4a-21f843472b82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/')\n",
        "#print(os.getcwd())\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader # 데이터로더\n",
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "\n",
        "# model 업그레이드로 인해 수정\n",
        "from minegpt2.kogpt2.utils import get_tokenizer\n",
        "from minegpt2.kogpt2.utils import download, tokenizer\n",
        "#from NarrativeKoGPT2.model.torch_gpt2 import GPT2Config, GPT2LMHeadModel\n",
        "from NarrativeKoGPT2.util.data import NovelDataset\n",
        "import gluonnlp\n",
        "from tqdm import tqdm\n",
        "print(torch.cuda.device_count())  # GPU deviec count check\n",
        "\n",
        "'''\n",
        "from NarrativeKoGPT2.util.data import NovelDataset\n",
        "\n",
        "class NovelDataset(Dataset):\n",
        "  \"\"\"web novel dataset\"\"\"\n",
        "\n",
        "  def __init__(self, file_path,vocab,tokenizer):\n",
        "    self.file_path = file_path\n",
        "    self.data =[]\n",
        "    self.vocab =vocab\n",
        "    self.tokenizer = tokenizer\n",
        "    file = open(self.file_path, 'r', encoding='utf-8')\n",
        "\n",
        "    while True:\n",
        "      line = file.readline()\n",
        "      if not line:\n",
        "        break\n",
        "      toeknized_line = tokenizer(line[:-1])\n",
        "      index_of_words = vocab[toeknized_line]\n",
        "      # print(np.shape(index_of_words))\n",
        "      self.data.append(index_of_words)\n",
        "\n",
        "    # print(np.shape(self.data))\n",
        "\n",
        "    file.close()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  def __getitem__(self,index):\n",
        "    item = self.data[index]\n",
        "    # print(item)\n",
        "    return item\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQiTUqairVsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import subprocess\n",
        "\n",
        "def get_gpu_memory_map():\n",
        "    \"\"\"Get the current gpu usage.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    usage: dict\n",
        "        Keys are device ids as integers.\n",
        "        Values are memory usage as integers in MB.\n",
        "    \"\"\"\n",
        "    result = subprocess.check_output(\n",
        "        [\n",
        "            'nvidia-smi', '--query-gpu=memory.used',\n",
        "            '--format=csv,nounits,noheader'\n",
        "        ], encoding='utf-8')\n",
        "    # Convert lines into a dictionary\n",
        "    gpu_memory = [int(x) for x in result.strip().split('\\n')]\n",
        "    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
        "    return gpu_memory_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCj_ozSeGaA7",
        "colab_type": "text"
      },
      "source": [
        "저기 아래 savepath에서 저장할파일 이름 안바꿔주면 미리해논거날아갈수잇음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOZVYU2BI7Ax",
        "colab_type": "text"
      },
      "source": [
        "소설 파일에 공백인 줄들이 있어서 len(data)!=0 c추가\n",
        "하라는데로 하면 이런 에러가 뜨기 때문에 공백이있어서그런듯\n",
        "공백을 없애주거나 공백이면 안하는 식으로 바꿈\n",
        "수집하는 데이터마다 다른 에러가 뜰수있어서\n",
        " 수집하고 전처리를 해야할듯...\n",
        "\n",
        "#gpu로 돌리면 RuntimeError: invalid argument 2: non-empty vector or matrix #expected at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:31\n",
        "\n",
        "파일에 따라서\n",
        "이런 에러나는데,,,cpu로 돌리면 안남.....\n",
        "근데 cpu로 돌리면 너무 느림...\n",
        "그래서 걍 예외처리함..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29A3HorbtG_Y",
        "colab_type": "text"
      },
      "source": [
        "아래는 버스커버스커 노래 가사. except가 많이 일어나서 loss가 별로 안줄어든듯...\n",
        "아니면 한 문장이 너무 짧아서 그런걸수도있어서 문장들을 길게 만들고 buskerbusker2.txt로 만들어서 다시 학습해봄\n",
        "결과적으로 잘됨\n",
        "아래는 소설에 대한 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgS7ve2W7juB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from NarrativeKoGPT2.kogpt2.pytorch_kogpt2 import get_pytorch_kogpt2_model\n",
        "ctx= 'cuda'#'cuda' #'cpu' #학습 Device CPU or GPU. colab의 경우 GPU 사용\n",
        "cache_dir='/content/drive/My Drive/NarrativeKoGPT2/kogpt2/' # KoGPT-2 모델 다운로드 경로\n",
        "save_path = '/content/drive/My Drive/NarrativeKoGPT2/checkpoint/111.tar' # 학습 후 결과 데이터\n",
        "data_file_path = '/content/drive/My Drive/NarrativeKoGPT2/data/soseal1.txt' # 학습하고 싶은 데이터\n",
        "#use_cuda = True # Colab내 GPU 사용을 위한 값\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9OuBxL070ac",
        "colab_type": "code",
        "outputId": "f0f4d982-a5a4-4694-bd2b-1e9072d042f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "\n",
        "tok_path = get_tokenizer()\n",
        "model, vocab = get_pytorch_kogpt2_model(ctx,cache_dir)\n",
        "sentencepieceTokenizer = SentencepieceTokenizer(tok_path)\n",
        "device=torch.device(ctx)\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "epoch =200  # 학습 epoch\n",
        "learning_rate = 1e-5\n",
        "batch_size = 2\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "novel_dataset = NovelDataset(data_file_path, vocab,sentencepieceTokenizer)\n",
        "novel_data_loader = DataLoader(novel_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "print('KoGPT-2 Transfer Learning Start')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "1\n",
            "using cached model\n",
            "/content/drive/My Drive/NarrativeKoGPT2/kogpt2/pytorch_kogpt2_676e9bcfa7.params\n",
            "using cached model\n",
            "/content/drive/My Drive/NarrativeKoGPT2/kogpt2/kogpt2_news_wiki_ko_cased_818bfa919d.spiece\n",
            "KoGPT-2 Transfer Learning Start\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucr3svtQr27I",
        "colab_type": "code",
        "outputId": "7114f35f-0c90-4599-d972-5069964f68c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "except_cnt=0\n",
        "for epoch in range(epoch):\n",
        "  count = 0\n",
        "  for data in novel_data_loader:\n",
        "    try:\n",
        "      if len(data)!=0:\n",
        "        optimizer.zero_grad()\n",
        "        data = torch.stack(data) # list of Tensor로 구성되어 있기 때문에 list를 stack을 통해 변환해준다.\n",
        "\n",
        "        data= data.transpose(1,0)\n",
        "        data= data.to(ctx)\n",
        "    \n",
        "        outputs = model(data, labels=data)\n",
        "        loss, logits = outputs[:2]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if count %10 ==0:\n",
        "          print('epoch no.{} train no.{}  loss = {}' . format(epoch, count+1, loss))\n",
        "          # torch.save(model,save_path+'checkpoint_{}_{}.tar'.format(epoch,count))\n",
        "          # 추론 및 학습 재개를 위한 일반 체크포인트 저장하기\n",
        "        if (count >0 and count%100==0) or (len(data) < batch_size):\n",
        "          torch.save({\n",
        "            'epoch': epoch,\n",
        "            'train_no': count,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss':loss\n",
        "          }, save_path)\n",
        "\n",
        "        count += 1\n",
        "    except:\n",
        "      count +=1\n",
        "      print(\"except\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch no.0 train no.1  loss = 7.365644454956055\n",
            "epoch no.0 train no.11  loss = 6.281390190124512\n",
            "epoch no.0 train no.21  loss = 6.683767795562744\n",
            "epoch no.1 train no.1  loss = 5.3509955406188965\n",
            "epoch no.1 train no.11  loss = 5.621183395385742\n",
            "epoch no.1 train no.21  loss = 4.972174644470215\n",
            "epoch no.1 train no.31  loss = 4.745198726654053\n",
            "epoch no.2 train no.1  loss = 5.189884185791016\n",
            "epoch no.2 train no.11  loss = 5.464982032775879\n",
            "epoch no.2 train no.21  loss = 5.213677406311035\n",
            "epoch no.2 train no.31  loss = 4.879258155822754\n",
            "epoch no.3 train no.1  loss = 5.256425857543945\n",
            "epoch no.3 train no.11  loss = 4.724671840667725\n",
            "epoch no.3 train no.21  loss = 4.304839134216309\n",
            "epoch no.3 train no.31  loss = 4.341354846954346\n",
            "epoch no.4 train no.1  loss = 5.341293811798096\n",
            "epoch no.4 train no.11  loss = 4.164826393127441\n",
            "epoch no.4 train no.21  loss = 4.377991676330566\n",
            "epoch no.5 train no.1  loss = 3.991142988204956\n",
            "epoch no.5 train no.11  loss = 4.281426429748535\n",
            "epoch no.5 train no.21  loss = 4.293251037597656\n",
            "epoch no.6 train no.1  loss = 3.923402786254883\n",
            "epoch no.6 train no.11  loss = 3.535799980163574\n",
            "epoch no.6 train no.21  loss = 3.727811813354492\n",
            "epoch no.7 train no.1  loss = 3.8467304706573486\n",
            "epoch no.7 train no.11  loss = 4.016031265258789\n",
            "epoch no.7 train no.21  loss = 3.8099658489227295\n",
            "epoch no.7 train no.31  loss = 4.0283522605896\n",
            "epoch no.8 train no.1  loss = 4.194939613342285\n",
            "epoch no.8 train no.11  loss = 3.1415212154388428\n",
            "epoch no.8 train no.21  loss = 3.5295865535736084\n",
            "epoch no.9 train no.1  loss = 3.2494516372680664\n",
            "epoch no.9 train no.11  loss = 3.449591636657715\n",
            "epoch no.9 train no.21  loss = 3.1930787563323975\n",
            "epoch no.9 train no.31  loss = 4.01366662979126\n",
            "epoch no.10 train no.1  loss = 2.2125158309936523\n",
            "epoch no.10 train no.11  loss = 3.5692343711853027\n",
            "epoch no.10 train no.21  loss = 3.3165535926818848\n",
            "epoch no.10 train no.31  loss = 3.2181894779205322\n",
            "epoch no.11 train no.1  loss = 3.218562126159668\n",
            "epoch no.11 train no.11  loss = 3.92787766456604\n",
            "epoch no.11 train no.21  loss = 2.9172961711883545\n",
            "epoch no.11 train no.31  loss = 3.557115077972412\n",
            "epoch no.12 train no.1  loss = 3.138632297515869\n",
            "epoch no.12 train no.11  loss = 3.049954414367676\n",
            "epoch no.12 train no.21  loss = 3.191239356994629\n",
            "epoch no.13 train no.1  loss = 3.7453315258026123\n",
            "epoch no.13 train no.11  loss = 2.804863929748535\n",
            "epoch no.13 train no.21  loss = 2.849838972091675\n",
            "epoch no.13 train no.31  loss = 3.373459577560425\n",
            "epoch no.14 train no.1  loss = 2.913238763809204\n",
            "epoch no.14 train no.11  loss = 2.931378126144409\n",
            "epoch no.14 train no.21  loss = 3.119258165359497\n",
            "epoch no.14 train no.31  loss = 2.7866885662078857\n",
            "epoch no.15 train no.1  loss = 2.916247844696045\n",
            "epoch no.15 train no.11  loss = 3.2333149909973145\n",
            "epoch no.15 train no.21  loss = 2.8556160926818848\n",
            "epoch no.15 train no.31  loss = 3.198420286178589\n",
            "epoch no.16 train no.1  loss = 2.5355188846588135\n",
            "epoch no.16 train no.11  loss = 2.0552775859832764\n",
            "epoch no.16 train no.21  loss = 3.0694947242736816\n",
            "epoch no.16 train no.31  loss = 2.955320358276367\n",
            "epoch no.17 train no.1  loss = 2.2927613258361816\n",
            "epoch no.17 train no.11  loss = 4.15132999420166\n",
            "epoch no.17 train no.21  loss = 1.9063674211502075\n",
            "epoch no.17 train no.31  loss = 3.2196249961853027\n",
            "epoch no.18 train no.1  loss = 3.4265434741973877\n",
            "epoch no.18 train no.11  loss = 2.4288523197174072\n",
            "epoch no.18 train no.21  loss = 2.100767135620117\n",
            "epoch no.18 train no.31  loss = 2.346304178237915\n",
            "epoch no.19 train no.1  loss = 2.8423540592193604\n",
            "epoch no.19 train no.11  loss = 2.807319402694702\n",
            "epoch no.19 train no.21  loss = 3.04433012008667\n",
            "epoch no.20 train no.1  loss = 2.7900609970092773\n",
            "epoch no.20 train no.11  loss = 2.46061635017395\n",
            "epoch no.20 train no.21  loss = 3.2161898612976074\n",
            "epoch no.20 train no.31  loss = 2.1127078533172607\n",
            "epoch no.21 train no.1  loss = 2.658378839492798\n",
            "epoch no.21 train no.11  loss = 2.1645946502685547\n",
            "epoch no.21 train no.21  loss = 2.1285715103149414\n",
            "epoch no.22 train no.1  loss = 2.369861125946045\n",
            "epoch no.22 train no.11  loss = 3.0316805839538574\n",
            "epoch no.22 train no.21  loss = 2.9382035732269287\n",
            "epoch no.23 train no.1  loss = 2.5761020183563232\n",
            "epoch no.23 train no.11  loss = 1.606298565864563\n",
            "epoch no.23 train no.21  loss = 2.1586203575134277\n",
            "epoch no.23 train no.31  loss = 1.2297444343566895\n",
            "epoch no.24 train no.1  loss = 2.514408588409424\n",
            "epoch no.24 train no.11  loss = 1.6527578830718994\n",
            "epoch no.24 train no.21  loss = 1.6016590595245361\n",
            "epoch no.24 train no.31  loss = 2.9837679862976074\n",
            "epoch no.25 train no.1  loss = 3.289897918701172\n",
            "epoch no.25 train no.11  loss = 2.498354196548462\n",
            "epoch no.25 train no.21  loss = 2.608837127685547\n",
            "epoch no.26 train no.1  loss = 2.6743979454040527\n",
            "epoch no.26 train no.11  loss = 2.4082651138305664\n",
            "epoch no.26 train no.21  loss = 2.5577127933502197\n",
            "epoch no.26 train no.31  loss = 2.821499824523926\n",
            "epoch no.27 train no.1  loss = 2.9776089191436768\n",
            "epoch no.27 train no.11  loss = 1.9982372522354126\n",
            "epoch no.27 train no.21  loss = 1.8414605855941772\n",
            "epoch no.28 train no.1  loss = 1.7975187301635742\n",
            "epoch no.28 train no.11  loss = 2.5062355995178223\n",
            "epoch no.28 train no.21  loss = 2.1446194648742676\n",
            "epoch no.28 train no.31  loss = 3.9259560108184814\n",
            "epoch no.29 train no.1  loss = 1.9511233568191528\n",
            "epoch no.29 train no.11  loss = 2.1819005012512207\n",
            "epoch no.29 train no.21  loss = 2.921050786972046\n",
            "epoch no.29 train no.31  loss = 2.755249261856079\n",
            "epoch no.30 train no.1  loss = 2.2192888259887695\n",
            "epoch no.30 train no.11  loss = 2.355414390563965\n",
            "epoch no.30 train no.21  loss = 2.6206095218658447\n",
            "epoch no.31 train no.1  loss = 2.1252517700195312\n",
            "epoch no.31 train no.11  loss = 2.1127681732177734\n",
            "epoch no.31 train no.21  loss = 3.053701877593994\n",
            "epoch no.32 train no.1  loss = 1.3219293355941772\n",
            "epoch no.32 train no.11  loss = 2.0770206451416016\n",
            "epoch no.32 train no.21  loss = 1.7104058265686035\n",
            "epoch no.33 train no.1  loss = 2.4740207195281982\n",
            "epoch no.33 train no.11  loss = 2.686713933944702\n",
            "epoch no.33 train no.21  loss = 1.6633470058441162\n",
            "epoch no.34 train no.1  loss = 1.6160386800765991\n",
            "epoch no.34 train no.11  loss = 2.4455227851867676\n",
            "epoch no.34 train no.21  loss = 2.0175445079803467\n",
            "epoch no.34 train no.31  loss = 1.6972901821136475\n",
            "epoch no.35 train no.1  loss = 2.855144739151001\n",
            "epoch no.35 train no.11  loss = 1.651624321937561\n",
            "epoch no.35 train no.21  loss = 2.1557464599609375\n",
            "epoch no.36 train no.1  loss = 1.9556398391723633\n",
            "epoch no.36 train no.11  loss = 2.2702901363372803\n",
            "epoch no.36 train no.21  loss = 2.8685243129730225\n",
            "epoch no.36 train no.31  loss = 2.146075963973999\n",
            "epoch no.37 train no.1  loss = 1.928968071937561\n",
            "epoch no.37 train no.11  loss = 0.9032412767410278\n",
            "epoch no.37 train no.21  loss = 2.641789674758911\n",
            "epoch no.38 train no.1  loss = 2.243030071258545\n",
            "epoch no.38 train no.11  loss = 2.422736406326294\n",
            "epoch no.38 train no.21  loss = 0.8630448579788208\n",
            "epoch no.38 train no.31  loss = 2.1657392978668213\n",
            "epoch no.39 train no.1  loss = 2.0608701705932617\n",
            "epoch no.39 train no.11  loss = 1.5360006093978882\n",
            "epoch no.39 train no.21  loss = 1.3225657939910889\n",
            "epoch no.39 train no.31  loss = 2.3398325443267822\n",
            "epoch no.40 train no.1  loss = 1.4069448709487915\n",
            "epoch no.40 train no.11  loss = 1.7831403017044067\n",
            "epoch no.40 train no.21  loss = 1.406778335571289\n",
            "epoch no.41 train no.1  loss = 1.937399983406067\n",
            "epoch no.41 train no.11  loss = 2.323845148086548\n",
            "epoch no.41 train no.21  loss = 1.0116045475006104\n",
            "epoch no.42 train no.1  loss = 1.5094143152236938\n",
            "epoch no.42 train no.11  loss = 2.4017419815063477\n",
            "epoch no.42 train no.21  loss = 2.0417087078094482\n",
            "epoch no.43 train no.1  loss = 2.727051258087158\n",
            "epoch no.43 train no.11  loss = 2.0656790733337402\n",
            "epoch no.43 train no.21  loss = 1.6565672159194946\n",
            "epoch no.44 train no.1  loss = 2.304395914077759\n",
            "epoch no.44 train no.11  loss = 1.3702359199523926\n",
            "epoch no.44 train no.21  loss = 1.6643086671829224\n",
            "epoch no.45 train no.1  loss = 2.46354603767395\n",
            "epoch no.45 train no.11  loss = 2.3791074752807617\n",
            "epoch no.45 train no.21  loss = 0.8458456993103027\n",
            "epoch no.45 train no.31  loss = 1.755573034286499\n",
            "epoch no.46 train no.1  loss = 2.12180495262146\n",
            "epoch no.46 train no.11  loss = 1.5171895027160645\n",
            "epoch no.46 train no.21  loss = 2.550316572189331\n",
            "epoch no.47 train no.1  loss = 1.8803019523620605\n",
            "epoch no.47 train no.11  loss = 1.0311012268066406\n",
            "epoch no.47 train no.21  loss = 1.9407376050949097\n",
            "epoch no.48 train no.1  loss = 2.1022636890411377\n",
            "epoch no.48 train no.11  loss = 2.3727877140045166\n",
            "epoch no.48 train no.21  loss = 1.5741770267486572\n",
            "epoch no.49 train no.1  loss = 1.3144152164459229\n",
            "epoch no.49 train no.11  loss = 1.2706352472305298\n",
            "epoch no.49 train no.21  loss = 1.4356167316436768\n",
            "epoch no.49 train no.31  loss = 2.206089973449707\n",
            "epoch no.50 train no.1  loss = 1.0251209735870361\n",
            "epoch no.50 train no.11  loss = 1.6585726737976074\n",
            "epoch no.50 train no.21  loss = 1.7120963335037231\n",
            "epoch no.50 train no.31  loss = 1.784654140472412\n",
            "epoch no.51 train no.1  loss = 1.2171999216079712\n",
            "epoch no.51 train no.11  loss = 1.1954268217086792\n",
            "epoch no.51 train no.21  loss = 2.6067748069763184\n",
            "epoch no.51 train no.31  loss = 1.810316801071167\n",
            "epoch no.52 train no.1  loss = 1.420235514640808\n",
            "epoch no.52 train no.11  loss = 1.7552697658538818\n",
            "epoch no.52 train no.21  loss = 2.2434070110321045\n",
            "epoch no.52 train no.31  loss = 1.4371856451034546\n",
            "epoch no.53 train no.1  loss = 1.6463415622711182\n",
            "epoch no.53 train no.11  loss = 1.2624131441116333\n",
            "epoch no.53 train no.21  loss = 1.3641753196716309\n",
            "epoch no.54 train no.1  loss = 1.7765260934829712\n",
            "epoch no.54 train no.11  loss = 1.5355125665664673\n",
            "epoch no.54 train no.21  loss = 1.094895839691162\n",
            "epoch no.54 train no.31  loss = 2.0895707607269287\n",
            "epoch no.55 train no.1  loss = 1.9308528900146484\n",
            "epoch no.55 train no.11  loss = 2.1601643562316895\n",
            "epoch no.55 train no.21  loss = 1.972173810005188\n",
            "epoch no.56 train no.1  loss = 1.1699622869491577\n",
            "epoch no.56 train no.11  loss = 1.6550105810165405\n",
            "epoch no.56 train no.21  loss = 1.8545726537704468\n",
            "epoch no.56 train no.31  loss = 1.2723143100738525\n",
            "epoch no.57 train no.1  loss = 1.6076663732528687\n",
            "epoch no.57 train no.11  loss = 1.9029490947723389\n",
            "epoch no.57 train no.21  loss = 0.7274020910263062\n",
            "epoch no.58 train no.1  loss = 1.3324387073516846\n",
            "epoch no.58 train no.11  loss = 0.8962559103965759\n",
            "epoch no.58 train no.21  loss = 1.6670855283737183\n",
            "epoch no.59 train no.1  loss = 1.5358351469039917\n",
            "epoch no.59 train no.11  loss = 1.3202602863311768\n",
            "epoch no.59 train no.21  loss = 1.4320412874221802\n",
            "epoch no.59 train no.31  loss = 1.0687669515609741\n",
            "epoch no.60 train no.1  loss = 1.3317596912384033\n",
            "epoch no.60 train no.11  loss = 1.12659752368927\n",
            "epoch no.60 train no.21  loss = 1.074010968208313\n",
            "epoch no.60 train no.31  loss = 0.9235730171203613\n",
            "epoch no.61 train no.1  loss = 1.9348440170288086\n",
            "epoch no.61 train no.11  loss = 0.7933278679847717\n",
            "epoch no.61 train no.21  loss = 1.0631797313690186\n",
            "epoch no.61 train no.31  loss = 2.9634995460510254\n",
            "epoch no.62 train no.1  loss = 1.961238980293274\n",
            "epoch no.62 train no.11  loss = 1.131483793258667\n",
            "epoch no.62 train no.21  loss = 1.2706936597824097\n",
            "epoch no.62 train no.31  loss = 1.3715295791625977\n",
            "epoch no.63 train no.1  loss = 1.4372600317001343\n",
            "epoch no.63 train no.11  loss = 1.1094417572021484\n",
            "epoch no.63 train no.21  loss = 1.4915083646774292\n",
            "epoch no.64 train no.1  loss = 0.6591349244117737\n",
            "epoch no.64 train no.11  loss = 2.180170774459839\n",
            "epoch no.64 train no.21  loss = 1.5791031122207642\n",
            "epoch no.65 train no.1  loss = 1.5342233180999756\n",
            "epoch no.65 train no.11  loss = 0.751772403717041\n",
            "epoch no.65 train no.21  loss = 1.4960182905197144\n",
            "epoch no.65 train no.31  loss = 1.3056026697158813\n",
            "epoch no.66 train no.1  loss = 1.0184422731399536\n",
            "epoch no.66 train no.11  loss = 2.403371572494507\n",
            "epoch no.66 train no.21  loss = 1.4373359680175781\n",
            "epoch no.67 train no.1  loss = 1.4024286270141602\n",
            "epoch no.67 train no.11  loss = 1.9808920621871948\n",
            "epoch no.67 train no.21  loss = 1.1722676753997803\n",
            "epoch no.68 train no.1  loss = 1.4058061838150024\n",
            "epoch no.68 train no.11  loss = 1.905861735343933\n",
            "epoch no.68 train no.21  loss = 0.8783560991287231\n",
            "epoch no.68 train no.31  loss = 1.0432342290878296\n",
            "epoch no.69 train no.1  loss = 1.489081621170044\n",
            "epoch no.69 train no.11  loss = 1.2215824127197266\n",
            "epoch no.69 train no.21  loss = 0.9735636711120605\n",
            "epoch no.69 train no.31  loss = 0.9239861369132996\n",
            "epoch no.70 train no.1  loss = 1.1916264295578003\n",
            "epoch no.70 train no.11  loss = 1.1464872360229492\n",
            "epoch no.70 train no.21  loss = 2.2434027194976807\n",
            "epoch no.71 train no.1  loss = 0.8351346254348755\n",
            "epoch no.71 train no.11  loss = 1.2624331712722778\n",
            "epoch no.71 train no.21  loss = 1.7393865585327148\n",
            "epoch no.72 train no.1  loss = 0.7827825546264648\n",
            "epoch no.72 train no.11  loss = 1.0756232738494873\n",
            "epoch no.72 train no.21  loss = 1.5956217050552368\n",
            "epoch no.72 train no.31  loss = 1.7084490060806274\n",
            "epoch no.73 train no.1  loss = 1.5499045848846436\n",
            "epoch no.73 train no.11  loss = 1.0598009824752808\n",
            "epoch no.73 train no.21  loss = 1.6901935338974\n",
            "epoch no.73 train no.31  loss = 1.9654295444488525\n",
            "epoch no.74 train no.1  loss = 1.0720126628875732\n",
            "epoch no.74 train no.11  loss = 2.6188721656799316\n",
            "epoch no.74 train no.21  loss = 0.9642592668533325\n",
            "epoch no.74 train no.31  loss = 0.9937078952789307\n",
            "epoch no.75 train no.1  loss = 0.7739696502685547\n",
            "epoch no.75 train no.11  loss = 2.691693067550659\n",
            "epoch no.75 train no.21  loss = 1.372613787651062\n",
            "epoch no.76 train no.1  loss = 2.327561378479004\n",
            "epoch no.76 train no.11  loss = 2.347252368927002\n",
            "epoch no.76 train no.21  loss = 0.6795830130577087\n",
            "epoch no.76 train no.31  loss = 1.2859392166137695\n",
            "epoch no.77 train no.1  loss = 1.1364270448684692\n",
            "epoch no.77 train no.11  loss = 1.7437481880187988\n",
            "epoch no.77 train no.21  loss = 1.2044422626495361\n",
            "epoch no.77 train no.31  loss = 1.5781500339508057\n",
            "epoch no.78 train no.1  loss = 0.5451540946960449\n",
            "epoch no.78 train no.11  loss = 1.282117247581482\n",
            "epoch no.78 train no.21  loss = 2.018486261367798\n",
            "epoch no.78 train no.31  loss = 3.075084686279297\n",
            "epoch no.79 train no.1  loss = 0.7689211368560791\n",
            "epoch no.79 train no.11  loss = 0.73801189661026\n",
            "epoch no.79 train no.21  loss = 1.3770583868026733\n",
            "epoch no.80 train no.1  loss = 0.8694933652877808\n",
            "epoch no.80 train no.11  loss = 1.686253547668457\n",
            "epoch no.80 train no.21  loss = 1.3917055130004883\n",
            "epoch no.81 train no.1  loss = 1.1764649152755737\n",
            "epoch no.81 train no.11  loss = 0.9484915733337402\n",
            "epoch no.81 train no.21  loss = 1.1558393239974976\n",
            "epoch no.81 train no.31  loss = 2.1591427326202393\n",
            "epoch no.82 train no.1  loss = 0.8332151174545288\n",
            "epoch no.82 train no.11  loss = 0.9205012321472168\n",
            "epoch no.82 train no.21  loss = 1.4638086557388306\n",
            "epoch no.82 train no.31  loss = 0.9903827905654907\n",
            "epoch no.83 train no.1  loss = 1.260728120803833\n",
            "epoch no.83 train no.11  loss = 0.9399104714393616\n",
            "epoch no.83 train no.21  loss = 2.197538375854492\n",
            "epoch no.84 train no.1  loss = 1.0320085287094116\n",
            "epoch no.84 train no.11  loss = 0.9803743958473206\n",
            "epoch no.84 train no.21  loss = 1.3364485502243042\n",
            "epoch no.84 train no.31  loss = 1.2147406339645386\n",
            "epoch no.85 train no.1  loss = 0.707560658454895\n",
            "epoch no.85 train no.11  loss = 1.7147401571273804\n",
            "epoch no.85 train no.21  loss = 1.077770709991455\n",
            "epoch no.86 train no.1  loss = 1.13629150390625\n",
            "epoch no.86 train no.11  loss = 1.1251227855682373\n",
            "epoch no.86 train no.21  loss = 0.8121578097343445\n",
            "epoch no.86 train no.31  loss = 1.2131152153015137\n",
            "epoch no.87 train no.1  loss = 1.328426718711853\n",
            "epoch no.87 train no.11  loss = 1.5459332466125488\n",
            "epoch no.87 train no.21  loss = 0.9431794881820679\n",
            "epoch no.87 train no.31  loss = 0.9979177117347717\n",
            "epoch no.88 train no.1  loss = 1.5479815006256104\n",
            "epoch no.88 train no.11  loss = 1.7935117483139038\n",
            "epoch no.88 train no.21  loss = 1.1087039709091187\n",
            "epoch no.88 train no.31  loss = 2.0822765827178955\n",
            "epoch no.89 train no.1  loss = 0.7373656630516052\n",
            "epoch no.89 train no.11  loss = 0.7858208417892456\n",
            "epoch no.89 train no.21  loss = 0.9180801510810852\n",
            "epoch no.90 train no.1  loss = 0.8235777616500854\n",
            "epoch no.90 train no.11  loss = 0.8291333913803101\n",
            "epoch no.90 train no.21  loss = 0.8856971263885498\n",
            "epoch no.90 train no.31  loss = 0.7916474938392639\n",
            "epoch no.91 train no.1  loss = 1.332184076309204\n",
            "epoch no.91 train no.11  loss = 1.553231120109558\n",
            "epoch no.91 train no.21  loss = 2.122896671295166\n",
            "epoch no.91 train no.31  loss = 1.909032940864563\n",
            "epoch no.92 train no.1  loss = 2.021585702896118\n",
            "epoch no.92 train no.11  loss = 0.5759274363517761\n",
            "epoch no.92 train no.21  loss = 0.5677489638328552\n",
            "epoch no.92 train no.31  loss = 1.7168394327163696\n",
            "epoch no.93 train no.1  loss = 0.7342557311058044\n",
            "epoch no.93 train no.11  loss = 1.0078974962234497\n",
            "epoch no.93 train no.21  loss = 0.9553794860839844\n",
            "epoch no.94 train no.1  loss = 0.6872208714485168\n",
            "epoch no.94 train no.11  loss = 1.6084887981414795\n",
            "epoch no.94 train no.21  loss = 0.5429679155349731\n",
            "epoch no.95 train no.1  loss = 1.9499151706695557\n",
            "epoch no.95 train no.11  loss = 1.2573145627975464\n",
            "epoch no.95 train no.21  loss = 1.011877417564392\n",
            "epoch no.95 train no.31  loss = 0.9776309728622437\n",
            "epoch no.96 train no.1  loss = 0.5645312666893005\n",
            "epoch no.96 train no.11  loss = 1.3557223081588745\n",
            "epoch no.96 train no.21  loss = 1.19456946849823\n",
            "epoch no.97 train no.1  loss = 0.8400348424911499\n",
            "epoch no.97 train no.11  loss = 0.9298533797264099\n",
            "epoch no.97 train no.21  loss = 2.560474157333374\n",
            "epoch no.97 train no.31  loss = 2.0577704906463623\n",
            "epoch no.98 train no.1  loss = 0.8705148696899414\n",
            "epoch no.98 train no.11  loss = 1.0710431337356567\n",
            "epoch no.98 train no.21  loss = 1.7104381322860718\n",
            "epoch no.99 train no.1  loss = 0.7448379993438721\n",
            "epoch no.99 train no.11  loss = 0.7690753936767578\n",
            "epoch no.99 train no.21  loss = 0.8453165292739868\n",
            "epoch no.100 train no.1  loss = 1.4275985956192017\n",
            "epoch no.100 train no.11  loss = 0.930976152420044\n",
            "epoch no.100 train no.21  loss = 1.4716746807098389\n",
            "epoch no.100 train no.31  loss = 1.0570068359375\n",
            "epoch no.101 train no.1  loss = 0.7091146111488342\n",
            "epoch no.101 train no.11  loss = 1.1430543661117554\n",
            "epoch no.101 train no.21  loss = 1.1539380550384521\n",
            "epoch no.101 train no.31  loss = 0.7619777321815491\n",
            "epoch no.102 train no.1  loss = 0.8215983510017395\n",
            "epoch no.102 train no.11  loss = 0.9507985711097717\n",
            "epoch no.102 train no.21  loss = 0.6652065515518188\n",
            "epoch no.103 train no.1  loss = 1.8431036472320557\n",
            "epoch no.103 train no.11  loss = 0.8096129894256592\n",
            "epoch no.103 train no.21  loss = 0.6464640498161316\n",
            "epoch no.104 train no.1  loss = 0.9135811924934387\n",
            "epoch no.104 train no.11  loss = 0.7205812931060791\n",
            "epoch no.104 train no.21  loss = 1.1954100131988525\n",
            "epoch no.104 train no.31  loss = 0.7201926708221436\n",
            "epoch no.105 train no.1  loss = 0.9812843203544617\n",
            "epoch no.105 train no.11  loss = 0.8331784605979919\n",
            "epoch no.105 train no.21  loss = 0.8234321475028992\n",
            "epoch no.106 train no.1  loss = 1.2643789052963257\n",
            "epoch no.106 train no.11  loss = 1.581375241279602\n",
            "epoch no.106 train no.21  loss = 0.7370004653930664\n",
            "epoch no.107 train no.1  loss = 2.1239168643951416\n",
            "epoch no.107 train no.11  loss = 1.0837105512619019\n",
            "epoch no.107 train no.21  loss = 0.7768707275390625\n",
            "epoch no.107 train no.31  loss = 1.4642411470413208\n",
            "epoch no.108 train no.1  loss = 1.199159860610962\n",
            "epoch no.108 train no.11  loss = 0.9267023801803589\n",
            "epoch no.108 train no.21  loss = 1.3534129858016968\n",
            "epoch no.108 train no.31  loss = 0.9937286972999573\n",
            "epoch no.109 train no.1  loss = 0.5683973431587219\n",
            "epoch no.109 train no.11  loss = 1.1303764581680298\n",
            "epoch no.109 train no.21  loss = 0.7619032263755798\n",
            "epoch no.110 train no.1  loss = 1.5887131690979004\n",
            "epoch no.110 train no.11  loss = 0.8458617329597473\n",
            "epoch no.110 train no.21  loss = 0.4964788854122162\n",
            "epoch no.110 train no.31  loss = 1.6867018938064575\n",
            "epoch no.111 train no.1  loss = 0.6163942813873291\n",
            "epoch no.111 train no.11  loss = 0.7557998299598694\n",
            "epoch no.111 train no.21  loss = 1.3146843910217285\n",
            "epoch no.111 train no.31  loss = 1.5548079013824463\n",
            "epoch no.112 train no.1  loss = 0.8368106484413147\n",
            "epoch no.112 train no.11  loss = 0.7937379479408264\n",
            "epoch no.112 train no.21  loss = 0.7854372262954712\n",
            "epoch no.112 train no.31  loss = 0.9247614741325378\n",
            "epoch no.113 train no.1  loss = 1.1817123889923096\n",
            "epoch no.113 train no.11  loss = 0.738943338394165\n",
            "epoch no.113 train no.21  loss = 1.2913622856140137\n",
            "epoch no.113 train no.31  loss = 0.6767067313194275\n",
            "epoch no.114 train no.1  loss = 1.861143708229065\n",
            "epoch no.114 train no.11  loss = 0.8492136597633362\n",
            "epoch no.114 train no.21  loss = 1.0101573467254639\n",
            "epoch no.115 train no.1  loss = 0.7612662315368652\n",
            "epoch no.115 train no.11  loss = 0.7559731602668762\n",
            "epoch no.115 train no.21  loss = 1.0700974464416504\n",
            "epoch no.116 train no.1  loss = 0.8703182935714722\n",
            "epoch no.116 train no.11  loss = 0.8016633987426758\n",
            "epoch no.116 train no.21  loss = 0.7545363903045654\n",
            "epoch no.116 train no.31  loss = 2.0810816287994385\n",
            "epoch no.117 train no.1  loss = 0.649386465549469\n",
            "epoch no.117 train no.11  loss = 0.5617473125457764\n",
            "epoch no.117 train no.21  loss = 0.7064095139503479\n",
            "epoch no.117 train no.31  loss = 2.5098965167999268\n",
            "epoch no.118 train no.1  loss = 0.8121846914291382\n",
            "epoch no.118 train no.11  loss = 0.7919166684150696\n",
            "epoch no.118 train no.21  loss = 0.7201894521713257\n",
            "epoch no.118 train no.31  loss = 2.914466619491577\n",
            "epoch no.119 train no.1  loss = 0.7056541442871094\n",
            "epoch no.119 train no.11  loss = 0.7392529845237732\n",
            "epoch no.119 train no.21  loss = 0.9950448274612427\n",
            "epoch no.119 train no.31  loss = 1.344604730606079\n",
            "epoch no.120 train no.1  loss = 1.1983957290649414\n",
            "epoch no.120 train no.11  loss = 0.661968469619751\n",
            "epoch no.120 train no.21  loss = 0.6376551985740662\n",
            "epoch no.120 train no.31  loss = 0.8237360715866089\n",
            "epoch no.121 train no.1  loss = 0.6540698409080505\n",
            "epoch no.121 train no.11  loss = 1.3303849697113037\n",
            "epoch no.121 train no.21  loss = 1.748826265335083\n",
            "epoch no.121 train no.31  loss = 0.6494950652122498\n",
            "epoch no.122 train no.1  loss = 0.8316442370414734\n",
            "epoch no.122 train no.11  loss = 0.7034092545509338\n",
            "epoch no.122 train no.21  loss = 1.029162049293518\n",
            "epoch no.123 train no.1  loss = 0.8679714202880859\n",
            "epoch no.123 train no.11  loss = 0.5896883606910706\n",
            "epoch no.123 train no.21  loss = 1.395219326019287\n",
            "epoch no.124 train no.1  loss = 0.8753760457038879\n",
            "epoch no.124 train no.11  loss = 0.7764707207679749\n",
            "epoch no.124 train no.21  loss = 0.7930888533592224\n",
            "epoch no.125 train no.1  loss = 0.8204024434089661\n",
            "epoch no.125 train no.11  loss = 0.7314443588256836\n",
            "epoch no.125 train no.21  loss = 0.7324801683425903\n",
            "epoch no.125 train no.31  loss = 1.6894989013671875\n",
            "epoch no.126 train no.1  loss = 0.5050193667411804\n",
            "epoch no.126 train no.11  loss = 0.9904541969299316\n",
            "epoch no.126 train no.21  loss = 0.9589609503746033\n",
            "epoch no.127 train no.1  loss = 0.5393319129943848\n",
            "epoch no.127 train no.11  loss = 0.44131729006767273\n",
            "epoch no.127 train no.21  loss = 0.5374929904937744\n",
            "epoch no.127 train no.31  loss = 1.2310458421707153\n",
            "epoch no.128 train no.1  loss = 1.4059789180755615\n",
            "epoch no.128 train no.11  loss = 0.9901338815689087\n",
            "epoch no.128 train no.21  loss = 1.2599601745605469\n",
            "epoch no.129 train no.1  loss = 0.44849103689193726\n",
            "epoch no.129 train no.11  loss = 1.714494228363037\n",
            "epoch no.129 train no.21  loss = 1.1874055862426758\n",
            "epoch no.130 train no.1  loss = 1.3678693771362305\n",
            "epoch no.130 train no.11  loss = 1.7639108896255493\n",
            "epoch no.130 train no.21  loss = 0.7978031635284424\n",
            "epoch no.130 train no.31  loss = 0.5204485654830933\n",
            "epoch no.131 train no.1  loss = 2.582881212234497\n",
            "epoch no.131 train no.11  loss = 0.7744234204292297\n",
            "epoch no.131 train no.21  loss = 0.7855014801025391\n",
            "epoch no.132 train no.1  loss = 1.5162606239318848\n",
            "epoch no.132 train no.11  loss = 0.5197645425796509\n",
            "epoch no.132 train no.21  loss = 0.4942595362663269\n",
            "epoch no.133 train no.1  loss = 1.3632303476333618\n",
            "epoch no.133 train no.11  loss = 2.3310627937316895\n",
            "epoch no.133 train no.21  loss = 0.47201797366142273\n",
            "epoch no.133 train no.31  loss = 0.5476032495498657\n",
            "epoch no.134 train no.1  loss = 0.5570753216743469\n",
            "epoch no.134 train no.11  loss = 0.935624361038208\n",
            "epoch no.134 train no.21  loss = 1.0955685377120972\n",
            "epoch no.135 train no.1  loss = 1.0258185863494873\n",
            "epoch no.135 train no.11  loss = 0.8935485482215881\n",
            "epoch no.135 train no.21  loss = 0.5000905990600586\n",
            "epoch no.135 train no.31  loss = 0.6078546047210693\n",
            "epoch no.136 train no.1  loss = 0.509611964225769\n",
            "epoch no.136 train no.11  loss = 1.0995917320251465\n",
            "epoch no.136 train no.21  loss = 0.653141975402832\n",
            "epoch no.136 train no.31  loss = 0.930569052696228\n",
            "epoch no.137 train no.1  loss = 0.7023577690124512\n",
            "epoch no.137 train no.11  loss = 1.0578968524932861\n",
            "epoch no.137 train no.21  loss = 0.7988824248313904\n",
            "epoch no.137 train no.31  loss = 0.883331835269928\n",
            "epoch no.138 train no.1  loss = 1.2601330280303955\n",
            "epoch no.138 train no.11  loss = 0.67857825756073\n",
            "epoch no.138 train no.21  loss = 0.6764369010925293\n",
            "epoch no.139 train no.1  loss = 1.2846771478652954\n",
            "epoch no.139 train no.11  loss = 0.6545645594596863\n",
            "epoch no.139 train no.21  loss = 0.3483230173587799\n",
            "epoch no.140 train no.1  loss = 1.6425302028656006\n",
            "epoch no.140 train no.11  loss = 0.5050755143165588\n",
            "epoch no.140 train no.21  loss = 0.635034441947937\n",
            "epoch no.141 train no.1  loss = 0.9438311457633972\n",
            "epoch no.141 train no.11  loss = 1.1004307270050049\n",
            "epoch no.141 train no.21  loss = 1.1962971687316895\n",
            "epoch no.141 train no.31  loss = 1.4587771892547607\n",
            "epoch no.142 train no.1  loss = 0.6914956569671631\n",
            "epoch no.142 train no.11  loss = 0.8242893218994141\n",
            "epoch no.142 train no.21  loss = 0.6807272434234619\n",
            "epoch no.143 train no.1  loss = 0.511203408241272\n",
            "epoch no.143 train no.11  loss = 0.6723884344100952\n",
            "epoch no.143 train no.21  loss = 0.962864339351654\n",
            "epoch no.143 train no.31  loss = 0.6775192022323608\n",
            "epoch no.144 train no.1  loss = 0.7052019834518433\n",
            "epoch no.144 train no.11  loss = 0.5694419145584106\n",
            "epoch no.144 train no.21  loss = 0.8770524859428406\n",
            "epoch no.144 train no.31  loss = 0.5664913654327393\n",
            "epoch no.145 train no.1  loss = 0.6265743374824524\n",
            "epoch no.145 train no.11  loss = 0.5764999985694885\n",
            "epoch no.145 train no.21  loss = 1.191636085510254\n",
            "epoch no.145 train no.31  loss = 0.40453842282295227\n",
            "epoch no.146 train no.1  loss = 0.628434956073761\n",
            "epoch no.146 train no.11  loss = 0.6162766814231873\n",
            "epoch no.146 train no.21  loss = 1.2489748001098633\n",
            "epoch no.147 train no.1  loss = 0.4541471302509308\n",
            "epoch no.147 train no.11  loss = 0.707856297492981\n",
            "epoch no.147 train no.21  loss = 0.6595009565353394\n",
            "epoch no.148 train no.1  loss = 0.6700079441070557\n",
            "epoch no.148 train no.11  loss = 0.5768498182296753\n",
            "epoch no.148 train no.21  loss = 0.9490627646446228\n",
            "epoch no.148 train no.31  loss = 1.1288375854492188\n",
            "epoch no.149 train no.1  loss = 0.5502615571022034\n",
            "epoch no.149 train no.11  loss = 0.69924396276474\n",
            "epoch no.149 train no.21  loss = 0.618174135684967\n",
            "epoch no.150 train no.1  loss = 1.0320019721984863\n",
            "epoch no.150 train no.11  loss = 0.6697041988372803\n",
            "epoch no.150 train no.21  loss = 0.6481719017028809\n",
            "epoch no.151 train no.1  loss = 1.977713942527771\n",
            "epoch no.151 train no.11  loss = 0.6291487216949463\n",
            "epoch no.151 train no.21  loss = 2.212214231491089\n",
            "epoch no.151 train no.31  loss = 0.6819594502449036\n",
            "epoch no.152 train no.1  loss = 1.0897046327590942\n",
            "epoch no.152 train no.11  loss = 0.7887768745422363\n",
            "epoch no.152 train no.21  loss = 0.694920003414154\n",
            "epoch no.152 train no.31  loss = 0.4874117374420166\n",
            "epoch no.153 train no.1  loss = 0.6122128963470459\n",
            "epoch no.153 train no.11  loss = 2.258141279220581\n",
            "epoch no.153 train no.21  loss = 0.6341885328292847\n",
            "epoch no.153 train no.31  loss = 0.5532476305961609\n",
            "epoch no.154 train no.1  loss = 0.4838206470012665\n",
            "epoch no.154 train no.11  loss = 0.5973067283630371\n",
            "epoch no.154 train no.21  loss = 0.45723462104797363\n",
            "epoch no.155 train no.1  loss = 0.6424890756607056\n",
            "epoch no.155 train no.11  loss = 0.7906911373138428\n",
            "epoch no.155 train no.21  loss = 0.5914320349693298\n",
            "epoch no.156 train no.1  loss = 0.7220202684402466\n",
            "epoch no.156 train no.11  loss = 0.8061639666557312\n",
            "epoch no.156 train no.21  loss = 0.4561065435409546\n",
            "epoch no.156 train no.31  loss = 0.8534332513809204\n",
            "epoch no.157 train no.1  loss = 0.8593558073043823\n",
            "epoch no.157 train no.11  loss = 0.48610034584999084\n",
            "epoch no.157 train no.21  loss = 0.721582293510437\n",
            "epoch no.158 train no.1  loss = 0.5555005669593811\n",
            "epoch no.158 train no.11  loss = 0.7805502414703369\n",
            "epoch no.158 train no.21  loss = 0.44441160559654236\n",
            "epoch no.159 train no.1  loss = 0.7743086218833923\n",
            "epoch no.159 train no.11  loss = 0.9057078957557678\n",
            "epoch no.159 train no.21  loss = 0.8250762224197388\n",
            "epoch no.159 train no.31  loss = 0.4843725860118866\n",
            "epoch no.160 train no.1  loss = 0.5622379183769226\n",
            "epoch no.160 train no.11  loss = 1.4570178985595703\n",
            "epoch no.160 train no.21  loss = 1.8707811832427979\n",
            "epoch no.160 train no.31  loss = 0.7882822155952454\n",
            "epoch no.161 train no.1  loss = 0.7214756608009338\n",
            "epoch no.161 train no.11  loss = 0.6968153119087219\n",
            "epoch no.161 train no.21  loss = 0.6213958859443665\n",
            "epoch no.162 train no.1  loss = 0.4881775677204132\n",
            "epoch no.162 train no.11  loss = 0.45792892575263977\n",
            "epoch no.162 train no.21  loss = 0.7043725848197937\n",
            "epoch no.163 train no.1  loss = 0.508803129196167\n",
            "epoch no.163 train no.11  loss = 0.41044047474861145\n",
            "epoch no.163 train no.21  loss = 0.6646726727485657\n",
            "epoch no.163 train no.31  loss = 0.9523298740386963\n",
            "epoch no.164 train no.1  loss = 0.6097143888473511\n",
            "epoch no.164 train no.11  loss = 0.7710158824920654\n",
            "epoch no.164 train no.21  loss = 0.736691415309906\n",
            "epoch no.165 train no.1  loss = 0.46351686120033264\n",
            "epoch no.165 train no.11  loss = 1.7742571830749512\n",
            "epoch no.165 train no.21  loss = 0.5925312042236328\n",
            "epoch no.165 train no.31  loss = 1.547027587890625\n",
            "epoch no.166 train no.1  loss = 1.5907557010650635\n",
            "epoch no.166 train no.11  loss = 0.5054317712783813\n",
            "epoch no.166 train no.21  loss = 0.5778369307518005\n",
            "epoch no.167 train no.1  loss = 0.8298811316490173\n",
            "epoch no.167 train no.11  loss = 0.5217200517654419\n",
            "epoch no.167 train no.21  loss = 0.7467498779296875\n",
            "epoch no.167 train no.31  loss = 0.64540696144104\n",
            "epoch no.168 train no.1  loss = 0.6213741302490234\n",
            "epoch no.168 train no.11  loss = 0.6515081524848938\n",
            "epoch no.168 train no.21  loss = 0.4662940204143524\n",
            "epoch no.168 train no.31  loss = 1.2652616500854492\n",
            "epoch no.169 train no.1  loss = 1.0409982204437256\n",
            "epoch no.169 train no.11  loss = 0.5821808576583862\n",
            "epoch no.169 train no.21  loss = 0.8854910731315613\n",
            "epoch no.170 train no.1  loss = 1.5466734170913696\n",
            "epoch no.170 train no.11  loss = 0.7689692974090576\n",
            "epoch no.170 train no.21  loss = 0.42678049206733704\n",
            "epoch no.171 train no.1  loss = 0.9926953911781311\n",
            "epoch no.171 train no.11  loss = 0.5877658724784851\n",
            "epoch no.171 train no.21  loss = 0.5184121131896973\n",
            "epoch no.171 train no.31  loss = 0.8785690665245056\n",
            "epoch no.172 train no.1  loss = 0.7157177925109863\n",
            "epoch no.172 train no.11  loss = 0.44465455412864685\n",
            "epoch no.172 train no.21  loss = 0.44626230001449585\n",
            "epoch no.172 train no.31  loss = 0.6320298910140991\n",
            "epoch no.173 train no.1  loss = 0.7590868473052979\n",
            "epoch no.173 train no.11  loss = 0.7298331260681152\n",
            "epoch no.173 train no.21  loss = 1.8082633018493652\n",
            "epoch no.174 train no.1  loss = 0.606457531452179\n",
            "epoch no.174 train no.11  loss = 0.6877138018608093\n",
            "epoch no.174 train no.21  loss = 0.7719970941543579\n",
            "epoch no.175 train no.1  loss = 2.381113290786743\n",
            "epoch no.175 train no.11  loss = 0.5007422566413879\n",
            "epoch no.175 train no.21  loss = 1.930942177772522\n",
            "epoch no.176 train no.1  loss = 0.46576836705207825\n",
            "epoch no.176 train no.11  loss = 1.0950956344604492\n",
            "epoch no.176 train no.21  loss = 0.5993829965591431\n",
            "epoch no.177 train no.1  loss = 1.021115779876709\n",
            "epoch no.177 train no.11  loss = 1.1012877225875854\n",
            "epoch no.177 train no.21  loss = 0.6586523056030273\n",
            "epoch no.178 train no.1  loss = 0.4167582392692566\n",
            "epoch no.178 train no.11  loss = 0.6319457292556763\n",
            "epoch no.178 train no.21  loss = 0.5679333806037903\n",
            "epoch no.179 train no.1  loss = 1.54869544506073\n",
            "epoch no.179 train no.11  loss = 0.3734985888004303\n",
            "epoch no.179 train no.21  loss = 0.670625627040863\n",
            "epoch no.180 train no.1  loss = 0.5671027302742004\n",
            "epoch no.180 train no.11  loss = 0.49096882343292236\n",
            "epoch no.180 train no.21  loss = 0.624384880065918\n",
            "epoch no.181 train no.1  loss = 0.6100186109542847\n",
            "epoch no.181 train no.11  loss = 0.6944411396980286\n",
            "epoch no.181 train no.21  loss = 1.1697887182235718\n",
            "epoch no.182 train no.1  loss = 1.6026666164398193\n",
            "epoch no.182 train no.11  loss = 1.196993112564087\n",
            "epoch no.182 train no.21  loss = 0.567074179649353\n",
            "epoch no.183 train no.1  loss = 1.393707275390625\n",
            "epoch no.183 train no.11  loss = 1.0738232135772705\n",
            "epoch no.183 train no.21  loss = 0.6763062477111816\n",
            "epoch no.183 train no.31  loss = 0.5466757416725159\n",
            "epoch no.184 train no.1  loss = 0.6481146216392517\n",
            "epoch no.184 train no.11  loss = 0.45956650376319885\n",
            "epoch no.184 train no.21  loss = 0.3473157584667206\n",
            "epoch no.184 train no.31  loss = 0.3696294128894806\n",
            "epoch no.185 train no.1  loss = 0.32429075241088867\n",
            "epoch no.185 train no.11  loss = 1.2592251300811768\n",
            "epoch no.185 train no.21  loss = 0.46822425723075867\n",
            "epoch no.185 train no.31  loss = 0.936026930809021\n",
            "epoch no.186 train no.1  loss = 0.7530865669250488\n",
            "epoch no.186 train no.11  loss = 0.549439549446106\n",
            "epoch no.186 train no.21  loss = 0.5121877789497375\n",
            "epoch no.186 train no.31  loss = 0.5755565762519836\n",
            "epoch no.187 train no.1  loss = 1.4227317571640015\n",
            "epoch no.187 train no.11  loss = 0.4983729124069214\n",
            "epoch no.187 train no.21  loss = 0.6624736189842224\n",
            "epoch no.188 train no.1  loss = 1.1935911178588867\n",
            "epoch no.188 train no.11  loss = 1.3458800315856934\n",
            "epoch no.188 train no.21  loss = 1.0364371538162231\n",
            "epoch no.189 train no.1  loss = 0.7004302740097046\n",
            "epoch no.189 train no.11  loss = 0.6450164318084717\n",
            "epoch no.189 train no.21  loss = 0.5687301754951477\n",
            "epoch no.190 train no.1  loss = 0.4781233072280884\n",
            "epoch no.190 train no.11  loss = 0.6165629029273987\n",
            "epoch no.190 train no.21  loss = 0.38419240713119507\n",
            "epoch no.191 train no.1  loss = 0.4623850882053375\n",
            "epoch no.191 train no.11  loss = 0.6535546779632568\n",
            "epoch no.191 train no.21  loss = 0.4550316631793976\n",
            "epoch no.192 train no.1  loss = 0.4500141143798828\n",
            "epoch no.192 train no.11  loss = 0.41079574823379517\n",
            "epoch no.192 train no.21  loss = 0.7323483228683472\n",
            "epoch no.193 train no.1  loss = 0.8981415629386902\n",
            "epoch no.193 train no.11  loss = 0.5476247072219849\n",
            "epoch no.193 train no.21  loss = 0.5417314171791077\n",
            "epoch no.194 train no.1  loss = 0.366140753030777\n",
            "epoch no.194 train no.11  loss = 0.45715275406837463\n",
            "epoch no.194 train no.21  loss = 1.34479558467865\n",
            "epoch no.194 train no.31  loss = 2.6028244495391846\n",
            "epoch no.195 train no.1  loss = 0.6634014248847961\n",
            "epoch no.195 train no.11  loss = 0.9996578693389893\n",
            "epoch no.195 train no.21  loss = 0.7363272309303284\n",
            "epoch no.196 train no.1  loss = 0.7656551003456116\n",
            "epoch no.196 train no.11  loss = 0.9141712188720703\n",
            "epoch no.196 train no.21  loss = 0.3581969141960144\n",
            "epoch no.197 train no.1  loss = 0.900087833404541\n",
            "epoch no.197 train no.11  loss = 0.4377416670322418\n",
            "epoch no.197 train no.21  loss = 1.5759601593017578\n",
            "epoch no.198 train no.1  loss = 0.7368159294128418\n",
            "epoch no.198 train no.11  loss = 0.46454301476478577\n",
            "epoch no.198 train no.21  loss = 0.48007628321647644\n",
            "epoch no.199 train no.1  loss = 0.42632395029067993\n",
            "epoch no.199 train no.11  loss = 0.700641393661499\n",
            "epoch no.199 train no.21  loss = 0.46279633045196533\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-eonwqwtMGK",
        "colab_type": "text"
      },
      "source": [
        "아래는 다운받은 텍스트 소설. 로스가 잘 줄어들고 잘 학습된다"
      ]
    }
  ]
}